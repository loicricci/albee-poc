# ğŸ¤– AI & Agent Conversation Enhancements

## Overview

This document covers the advanced AI conversation features added to Gabee, focusing on:
- **GPT-4o integration** with streaming responses
- **Advanced context management** with semantic filtering
- **Conversation intelligence** with quality tracking
- **Semantic memory extraction** for long-term learning
- **Real-time streaming** via Server-Sent Events

---

## ğŸ¯ Key Features

### 1. **Streaming Responses** (NEW)
Real-time token-by-token responses using Server-Sent Events (SSE).

**Endpoint:** `POST /chat/stream`

**Features:**
- Real-time streaming of GPT-4o/4o-mini responses
- Compatible with all modern browsers
- Lower perceived latency for users
- Progress indicators during generation

**Usage:**
```javascript
const evtSource = new EventSource(
  `/chat/stream?conversation_id=${convId}&question=${encodeURIComponent(q)}`,
  { headers: { 'Authorization': `Bearer ${token}` }}
);

evtSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.token) {
    // Append token to UI
    appendToken(data.token);
  } else if (data.event === 'complete') {
    // Response complete
    evtSource.close();
  }
};
```

---

### 2. **GPT-4o Support** (NEW)
Upgraded from GPT-3.5-turbo to GPT-4o for better reasoning and context understanding.

**Benefits:**
- 128K context window (vs 16K)
- Better factual accuracy
- Improved instruction following
- More nuanced persona adherence
- Multimodal capabilities (future: image understanding)

**Toggle between models:**
```bash
POST /chat/ask-v2?use_gpt4o=true  # Use GPT-4o (higher cost, better quality)
POST /chat/ask-v2?use_gpt4o=false # Use GPT-4o-mini (default, faster, cheaper)
```

---

### 3. **Advanced Context Management** (NEW)

#### 3.1 Semantic Message Filtering
Intelligently selects most relevant messages from conversation history.

**How it works:**
1. Always keeps last 5 messages (recency)
2. Embeds older messages and current query
3. Calculates cosine similarity
4. Keeps top 10 most relevant historical messages
5. Maintains chronological order

**Benefits:**
- Handles conversations of any length
- Focuses on relevant context
- Reduces token usage
- Maintains conversation coherence

#### 3.2 Automatic Conversation Summarization
Creates summaries for long conversations (>50 messages).

**Storage:** `conversation_summaries` table
```sql
SELECT summary, created_at 
FROM conversation_summaries 
WHERE conversation_id = ?
```

**When summaries are used:**
- Injected into system prompt for long conversations
- Updated every hour for active conversations
- Helps maintain context without full history

#### 3.3 Dynamic Context Window
Adapts context size based on conversation length and complexity.

**Configuration (in `context_manager.py`):**
```python
self.max_context_messages = 20  # Maximum messages to keep
self.max_tokens = 8000          # Reserve tokens for context
self.summary_threshold = 50     # Summarize when conversation exceeds this
```

---

### 4. **Semantic Memory Extraction** (NEW)

Automatically extracts structured memories from conversations.

**Memory Types:**
- **Facts:** Concrete information (name, job, location)
- **Preferences:** Likes, dislikes, interests
- **Relationships:** Mentions of other people
- **Events:** Important events or plans

**Storage:** `avee_memories` table with vector embeddings

**Example:**
```json
{
  "type": "preference",
  "content": "User loves Python programming and FastAPI",
  "confidence": 0.9,
  "source_message_id": "uuid"
}
```

**Usage in conversations:**
Memories are automatically searched and injected into context:
```
STORED MEMORIES:
- User loves Python programming (confidence: 90%)
- User works as a software engineer (confidence: 95%)
- User is planning a trip to Japan (confidence: 80%)
```

**API Endpoints:**
```bash
# Get memories for a conversation's Avee
GET /chat/{conversation_id}/memories

# Disable memory extraction for a request
POST /chat/ask-v2?extract_memories=false
```

---

### 5. **Conversation Intelligence** (NEW)

#### 5.1 Turn Quality Analysis
Analyzes each conversation turn for quality.

**Storage:** `conversation_quality` table
```sql
SELECT 
  AVG(relevance_score) as avg_relevance,
  AVG(engagement_score) as avg_engagement,
  AVG(factual_grounding) as avg_grounding
FROM conversation_quality
WHERE conversation_id = ?
```

#### 5.2 Topic Extraction
Identifies main topics in conversations.

**Example:**
```json
{
  "topics": [
    "Python programming",
    "FastAPI development",
    "career advice"
  ]
}
```

#### 5.3 Follow-up Suggestions
AI generates smart follow-up questions for users.

**Example response:**
```json
{
  "follow_up_suggestions": [
    "What are some best practices for FastAPI error handling?",
    "How do you structure large FastAPI projects?",
    "Can you recommend resources for learning async Python?"
  ]
}
```

#### 5.4 Automatic Title Generation
Creates concise titles for conversations.

**When:** After 4+ messages in a conversation
**Storage:** `conversations.title` column

---

### 6. **Enhanced Endpoints**

#### 6.1 `/chat/ask-v2` (NEW)
Enhanced version of `/chat/ask` with all new features.

**Request:**
```bash
POST /chat/ask-v2?conversation_id={uuid}&question={text}&use_gpt4o=true&extract_memories=true
Authorization: Bearer {token}
```

**Response:**
```json
{
  "conversation_id": "uuid",
  "layer_used": "friends",
  "answer": "Here's my response...",
  "model_used": "gpt-4o",
  "used_chunks": 5,
  "used_persona": true,
  "quality_scores": {
    "relevance_score": 0.95,
    "engagement_score": 0.88,
    "factual_grounding": 0.92,
    "issues": [],
    "suggestions": ["Consider adding more examples"]
  },
  "topics": ["programming", "career"],
  "follow_up_suggestions": [
    "What about X?",
    "How does Y work?",
    "Can you explain Z?"
  ],
  "memories_extracted": 3,
  "conversation_title": "Python Career Discussion"
}
```

#### 6.2 `/chat/stream` (NEW)
Streaming version with Server-Sent Events.

**Request:**
```bash
POST /chat/stream?conversation_id={uuid}&question={text}&use_gpt4o=false
Authorization: Bearer {token}
```

**Response Stream:**
```
data: {"event": "start", "model": "gpt-4o-mini"}

data: {"token": "Hello"}

data: {"token": "! "}

data: {"token": "How"}

data: {"token": " can"}

...

data: {"event": "complete", "message_id": "uuid", "total_tokens": 150}
```

#### 6.3 `/chat/{conversation_id}/intelligence` (NEW)
Get conversation analytics.

**Response:**
```json
{
  "conversation_id": "uuid",
  "title": "Python Discussion",
  "metrics": {
    "total_turns": 15,
    "avg_user_message_length": 127.5,
    "avg_response_time": 2.3,
    "conversation_depth": 30,
    "user_satisfaction_estimate": 0.85
  },
  "topics": ["python", "fastapi", "deployment"],
  "average_quality": {
    "relevance": 92,
    "engagement": 87,
    "factual_grounding": 94
  },
  "total_messages": 30
}
```

#### 6.4 `/chat/{conversation_id}/memories` (NEW)
Get extracted memories for an Avee.

**Response:**
```json
{
  "avee_id": "uuid",
  "memories": [
    {
      "type": "fact",
      "content": "User is a software engineer at Google",
      "confidence": 0.95,
      "created_at": "2024-12-21T10:30:00Z"
    },
    {
      "type": "preference",
      "content": "User prefers Python over JavaScript",
      "confidence": 0.88,
      "created_at": "2024-12-21T10:35:00Z"
    }
  ],
  "total": 2
}
```

---

## ğŸ—„ï¸ Database Schema

### New Tables

#### `conversation_summaries`
```sql
CREATE TABLE conversation_summaries (
    id UUID PRIMARY KEY,
    conversation_id UUID REFERENCES conversations(id),
    summary TEXT NOT NULL,
    messages_included INTEGER,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### `avee_memories`
```sql
CREATE TABLE avee_memories (
    id UUID PRIMARY KEY,
    avee_id UUID REFERENCES avees(id),
    memory_type VARCHAR(50) CHECK (memory_type IN ('fact', 'preference', 'relationship', 'event')),
    content TEXT NOT NULL,
    confidence_score INTEGER CHECK (confidence_score >= 0 AND confidence_score <= 100),
    source_message_id UUID REFERENCES messages(id),
    embedding vector(1536),  -- For semantic search
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### `conversation_quality`
```sql
CREATE TABLE conversation_quality (
    id UUID PRIMARY KEY,
    conversation_id UUID REFERENCES conversations(id),
    message_id UUID REFERENCES messages(id),
    relevance_score INTEGER,
    engagement_score INTEGER,
    factual_grounding INTEGER,
    issues TEXT,  -- JSON array
    suggestions TEXT,  -- JSON array
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### New Columns

#### `avees.persona_notes`
```sql
ALTER TABLE avees ADD COLUMN persona_notes TEXT;
```

---

## ğŸš€ Setup & Migration

### 1. Install Dependencies
```bash
cd backend
pip install -r requirements.txt
```

**New dependencies:**
- `numpy>=1.24.0` - Cosine similarity calculations
- `sentence-transformers>=2.2.2` - Already installed for reranking

### 2. Run Database Migration
```bash
# Option 1: Run SQL directly
psql $DATABASE_URL < backend/migrations/001_add_ai_features.sql

# Option 2: Use Python migration runner
python -m backend.run_migration
```

### 3. Restart Backend
```bash
uvicorn backend.main:app --reload --port 8000
```

### 4. Test New Endpoints
```bash
# Test streaming
curl -N -H "Authorization: Bearer $TOKEN" \
  "http://localhost:8000/chat/stream?conversation_id=$CONV_ID&question=Hello"

# Test enhanced chat with GPT-4o
curl -X POST -H "Authorization: Bearer $TOKEN" \
  "http://localhost:8000/chat/ask-v2?conversation_id=$CONV_ID&question=Hello&use_gpt4o=true"

# Get conversation intelligence
curl -H "Authorization: Bearer $TOKEN" \
  "http://localhost:8000/chat/$CONV_ID/intelligence"
```

---

## ğŸ“Š Performance Considerations

### Token Usage
- **GPT-4o:** ~$0.005/1K input tokens, ~$0.015/1K output tokens
- **GPT-4o-mini:** ~$0.00015/1K input tokens, ~$0.0006/1K output tokens
- **Embeddings:** ~$0.00002/1K tokens (text-embedding-3-small)

### Optimization Strategies
1. **Use GPT-4o-mini by default** - Switch to GPT-4o for complex queries
2. **Enable context filtering** - Reduces token usage by 50-70%
3. **Cache embeddings** - Reuse embeddings for repeated queries
4. **Batch memory extraction** - Extract memories async, not on every turn

### Latency
- **Standard response:** 2-5 seconds
- **Streaming start:** <1 second (first token)
- **Context filtering:** +0.1-0.3 seconds
- **Memory search:** +0.05-0.15 seconds

---

## ğŸ”§ Configuration

### Environment Variables
```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-...
EMBED_MODEL=text-embedding-3-small  # Default embedding model

# Reranker Configuration
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2  # Default reranker

# Database
DATABASE_URL=postgresql://...
```

### Context Manager Settings
Edit `backend/context_manager.py`:
```python
class ContextManager:
    def __init__(self, db, conversation_id, avee_id):
        # ... 
        self.max_context_messages = 20  # Increase for more context
        self.max_tokens = 8000          # Increase for GPT-4o's 128K window
        self.summary_threshold = 50     # Lower to summarize sooner
```

---

## ğŸ¨ Frontend Integration

### Streaming Chat Component (React/Next.js)

```typescript
// hooks/useStreamingChat.ts
import { useState, useCallback } from 'react';

export function useStreamingChat(conversationId: string) {
  const [streamingResponse, setStreamingResponse] = useState('');
  const [isStreaming, setIsStreaming] = useState(false);

  const sendMessage = useCallback(async (question: string, token: string) => {
    setIsStreaming(true);
    setStreamingResponse('');

    const url = `${process.env.NEXT_PUBLIC_API_BASE}/chat/stream?conversation_id=${conversationId}&question=${encodeURIComponent(question)}`;

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${token}`,
      }
    });

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader!.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = JSON.parse(line.slice(6));
          
          if (data.token) {
            setStreamingResponse(prev => prev + data.token);
          } else if (data.event === 'complete') {
            setIsStreaming(false);
          }
        }
      }
    }
  }, [conversationId]);

  return { streamingResponse, isStreaming, sendMessage };
}
```

### Enhanced Chat Component
```typescript
// components/EnhancedChat.tsx
'use client';

import { useState } from 'react';
import { useStreamingChat } from '@/hooks/useStreamingChat';

export function EnhancedChat({ conversationId }: { conversationId: string }) {
  const [input, setInput] = useState('');
  const [followUps, setFollowUps] = useState<string[]>([]);
  const { streamingResponse, isStreaming, sendMessage } = useStreamingChat(conversationId);

  const handleSend = async () => {
    const token = await getAccessToken();
    await sendMessage(input, token);
    
    // Get follow-up suggestions after response completes
    // (would need separate API call)
  };

  return (
    <div>
      {/* ... messages ... */}
      
      {isStreaming && (
        <div className="streaming-indicator">
          <div className="typing-dots">...</div>
          <div className="partial-response">{streamingResponse}</div>
        </div>
      )}
      
      {followUps.length > 0 && (
        <div className="follow-ups">
          <p>Suggested questions:</p>
          {followUps.map((q, i) => (
            <button key={i} onClick={() => setInput(q)}>
              {q}
            </button>
          ))}
        </div>
      )}
      
      <input 
        value={input}
        onChange={e => setInput(e.target.value)}
        disabled={isStreaming}
      />
      <button onClick={handleSend} disabled={isStreaming}>
        Send
      </button>
    </div>
  );
}
```

---

## ğŸ“ˆ Monitoring & Analytics

### Track Conversation Quality
```sql
-- Average quality scores over time
SELECT 
  DATE(created_at) as date,
  AVG(relevance_score) as avg_relevance,
  AVG(engagement_score) as avg_engagement,
  AVG(factual_grounding) as avg_grounding
FROM conversation_quality
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

### Track Memory Accumulation
```sql
-- Memories extracted per Avee
SELECT 
  a.handle,
  COUNT(m.id) as total_memories,
  AVG(m.confidence_score) as avg_confidence
FROM avees a
LEFT JOIN avee_memories m ON m.avee_id = a.id
GROUP BY a.id, a.handle
ORDER BY total_memories DESC;
```

### Track Model Usage
```sql
-- Add a model_used column to messages table
ALTER TABLE messages ADD COLUMN model_used VARCHAR(50);

-- Track costs
SELECT 
  model_used,
  COUNT(*) as total_messages,
  SUM(LENGTH(content)) as total_chars
FROM messages
WHERE role = 'assistant'
GROUP BY model_used;
```

---

## ğŸ¯ Next Steps

### Short Term
1. âœ… Streaming responses
2. âœ… GPT-4o integration
3. âœ… Context management
4. âœ… Memory extraction
5. â³ Frontend integration for streaming
6. â³ Cache embeddings with Redis

### Medium Term
1. Voice input/output with Whisper/TTS
2. Multi-modal support (image understanding with GPT-4o vision)
3. Proactive Avee behaviors (scheduled messages, digests)
4. A/B testing different personas
5. Real-time collaboration (multiple users chatting with same Avee)

### Long Term
1. Fine-tuned models per Avee
2. Federated learning across Avees
3. Cross-Avee knowledge sharing
4. Advanced reasoning with chain-of-thought
5. Multi-agent conversations

---

## ğŸ“ Summary

**What's New:**
- ğŸš€ **Streaming responses** for real-time interaction
- ğŸ§  **GPT-4o** for better reasoning and 128K context
- ğŸ¯ **Smart context management** with semantic filtering
- ğŸ’¾ **Semantic memory** that grows with conversations
- ğŸ“Š **Conversation intelligence** with quality tracking
- ğŸ’¡ **Follow-up suggestions** for better engagement
- ğŸ·ï¸ **Auto-generated titles** for organization

**Impact:**
- **User Experience:** Feels more alive and responsive
- **Quality:** Better, more relevant responses
- **Learning:** Avees remember and improve over time
- **Analytics:** Track and optimize conversation quality
- **Scalability:** Handles any conversation length

---

**Questions or issues?** Open an issue or check the API docs at `http://localhost:8000/docs`








